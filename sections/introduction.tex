%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.1, 2014-11-21

\chapter{Introduction}
\label{ch:Introduction}
Language Identification (LID) describes the classification task of differentiating between spoken speech in different languages and being able to correctly classify which speech-segments consists of which language.  Neural Networks refer to Artificial Neural Networks (ANNs), a Machine Learning approach to classification tasks. They are employed greatly throughout all sciences and especially in computer science and tasks concerned with the processing of speech. This thesis proposes a low-latency, fast, and ``online'', approach to Language Identification using ANNs.

The work done in this thesis uses the Janus Recognition Toolkit (JRTk)~\cite{woszczyna1994janus}, an Automatic Speech Recognition toolkit developed in joint cooperation by the KIT and CMU. The JRTk offers a tcl/tk\footnote{Tcl/tk:~\url{https://www.tcl.tk/}} script-based environment for the development of Automatic Speech Recognition (ASR) systems, therefore source code in this thesis will consist of tcl/tk scripts with (some) Janus-specific commands. The JRTk and tcl/tk are further explained in sec.~\ref{sec:fund:JRTk}.

\section{Motivation}
\label{sec:Introduction:Motivation}
ASR is used in many applications and devices today, especially in the rise of handheld mobile devices like smartphones and tablets. It has progressed quickly in the last five years and has found commercial success. Famous examples include Google's\footnote{Google: \url{www.google.com}} ``Ok, Google'' and Apple's\footnote{Apple: \url{www.apple.com}} Siri. Which both include voice search\cite{franz2008voice} and a form of voice control, that even is extensible in the case of Google and Android e.g\cite{voicecontrol2014}. Many other applications have emerged, including spoken language translation\footnote{IWSLT: \url{iwslt.org}}, especially relevant for this thesis in the realm of spoken language translation, in the form of the Lecture Translator\cite{lecturetranslator2016} .

The task of LID can be applied in all of those fields, as the best-performing ASR is trained on one language and therefore requires changing the trained model when the input language changes. This requires user interaction in the form of defining the input language. Robust and low-latency language identification could eliminate the need for this interaction and make the user experience more streamlined.

LID would be especially applicable in the realm of spoken language translation, as used for example in the European Parliament where already components of ASR and Machine Translation are employed and are being actively developed in the TC-STAR initiative\footnote{TC-STAR: \url{tcstar.org}}, e.g as in\cite{vilar2005statistical}, and LID would further be able to automate these translation tasks.

This thesis will focus mostly on the KIT's Lecture Translator\footnote{Lecture Translator: \url{https://lecture-translator.kit.edu}}  as the system trained was implemented for it. We believe our results are generic enough to be transferable to other applications: e.g. fully-automated European Parliament speech translations, or  with small implementation-specific changes.

\section{Overview}
\label{sec:Intro:Overview}

This thesis is set up as follows: Chapter 2 introduces related and previous work in the realm of Language Identification. Chapter 3 gives an introductory view of LID, and further defines the task this thesis tries to solve. Afterwards we give preliminary theoretical explanations and definitions, including an introduction to Neural Networks in Sec.~\ref{sec:fund:NN}. Chapter 4 describes the experimental setup used in this thesis, including the data corpora. Audio Preprocessing is explained in detail in chapter 5.

Chapters 6 describes our results that were accomplished by evaluation of various network setups and layouts, e.g. the amount of hidden layers and neurons. Different smoothing mechanisms on top of the direct neuronal output layer of the network are explained in chapter 7. This is followed by the final summary of our work and an outlook on future work in chapter 8.