%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.1, 2014-11-21

\chapter{Introduction}
\label{ch:Introduction}
Language Identification (LID) describes the classification task of differentiating between spoken speech in different languages and being able to correctly classify which speech-segments consists of which language.  Neural Networks refer to Artificial Neural Network's, a Machine Learning approach to classification tasks employed greatly throughout all sciences and especially in computer science and tasks concerned with the processing of spoken speech. This thesis tries to find a low-latency, fast, or ``online'', approach to Language Identification using the classification method of neural networks.

The work done in this thesis uses the Janus Recognition Toolkit (jrtk)\footnote{Janus Recognition Toolkit:~\url{http://isl.anthropomatik.kit.edu/cmu-kit/english/1406.php}}, an Automatic Speech Recognition toolkit developed in joint cooperation by the KIT and CMU. The jrtk offers a tcl/tk\footnote{Tcl/tk:~\url{https://www.tcl.tk/}} script-based environment for the development of Automatic Speech Recognition systems, therefore source code in this thesis will consist of tcl/tk scripts with (some) janus-specific commands. The jrtk and tcl/tk are further explained in sec.~\ref{sec:fund:jrtk}.

\section{Motivation}
\label{sec:Introduction:Motivation}
Automatic Speech Recognition (ASR) is used in many applications and devices today, especially in the rise of handheld mobile devices like smart-phones and tablets. It has progressed quickly in the last five years and has found commercial success. Famous examples include Google\footnote{Google: \url{www.google.com}}'s ``Ok, Google'' and Apple\footnote{Apple: \url{www.apple.com}}'s Siri. Which both include voice search\cite{franz2008voice} and a form of voice control, that even is extensible in the case of Google and Android e.g\cite{voicecontrol2014}. Many other applications have emerged, including spoken language translation\footnote{IWSLT: \url{iwslt.org}}, especially relevant for this thesis in the realm of Lecture Translation\cite{lecturetranslator2016} .

The task of Language Identification can be applied in all of those fields, as Automatic Speech Recognition is mostly trained on one language and therefore requires a totally different setup of classifiers per language, making a manual change of language previous to recognition necessary. Robust and low-latency language identification would eliminate the need for this.

LID would be especially applicable in the realm Spoken language translation, as used for example in the European Parliament where already components of ASR and Machine Translation are employed and are being actively developed\footnote{TC-STAR: \url{tcstar.org}}\cite{vilar2005statistical}, and LID would further be able to automate these translation tasks.

This thesis will focus mostly on the KIT's lecture Translator\footnote{Lecture Translator: \url{https://lecture-translator.kit.edu}}  as the system trained was implemented for it. We believe our results are general enough to be transferable to other applications with small implementation-specific changes.

\section{Overview}
\label{sec:Intro:Overview}

The following chapter gives an introductory view of the applications of Language Identification, and introduces the tasks this thesis tries to solve. Afterwards we give preliminary theoretical explanations and definitions, including an introduction to Neural Networks in Sec.~\ref{sec:fund:NN}. The next chapter describes the experimental setup used in this thesis, including the data corpusses and feature preprocessing done on raw audio files. 

Chapters 4 and 5 describe our results that were accomplished by trying out different Network Structures and geometries in chapter 4 as well as different smoothing mechanisms on top of the direct neuronal output layer of the network in chapter 5. This is followed by the final summary of our work and an outlook onto future work improving upon these results.